{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ee2984-28f9-4ad7-a8df-0db00220823f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in /opt/anaconda3/lib/python3.12/site-packages (6.0.12)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.12/site-packages (5.2.1)\n",
      "Requirement already satisfied: sgmllib3k in /opt/anaconda3/lib/python3.12/site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de545c4e6f774bb19f1c2e4dca5f521d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Th√®me :', layout=Layout(width='60%'), options=('Agriculture, alimentation, for√™t, bio√©co‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0880cdd2244987b49f3631ad19204e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Afficher les articles üì∞', style=ButtonStyle()), But‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87d99e1fc3c4856a3f940f02f2e76e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install feedparser requests beautifulsoup4 lxml\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Dictionnaire des flux ADEME\n",
    "# ----------------------------\n",
    "FEEDS = {\n",
    "    \"Agriculture, alimentation, for√™t, bio√©conomie\": \"https://librairie.ademe.fr/rss/3516-thematique-agriculture-alimentation-foret-bioeconomie.xml\",\n",
    "    \"Air\": \"https://librairie.ademe.fr/rss/3145-thematique-air.xml\",\n",
    "    \"B√¢timent\": \"https://librairie.ademe.fr/rss/3153-thematique-batiment.xml\",\n",
    "    \"Changement climatique\": \"https://librairie.ademe.fr/rss/3147-thematique-changement-climatique.xml\",\n",
    "    \"Consommer autrement\": \"https://librairie.ademe.fr/rss/2906-thematique-consommer-autrement.xml\",\n",
    "    \"√âconomie circulaire et D√©chets\": \"https://librairie.ademe.fr/rss/3426-thematique-economie-circulaire-et-dechets.xml\",\n",
    "    \"√ânergies\": \"https://librairie.ademe.fr/rss/3149-thematique-energies.xml\",\n",
    "    \"Industrie et production durable\": \"https://librairie.ademe.fr/rss/3503-thematique-industrie-et-production-durable.xml\",\n",
    "    \"Institutionnel\": \"https://librairie.ademe.fr/rss/3157-thematique-institutionnel.xml\",\n",
    "    \"Mobilit√© et transports\": \"https://librairie.ademe.fr/rss/2901-thematique-mobilite-et-transports.xml\",\n",
    "    \"Recherche et innovation\": \"https://librairie.ademe.fr/rss/2930-thematique-recherche-et-innovation.xml\",\n",
    "    \"Soci√©t√© et politiques publiques\": \"https://librairie.ademe.fr/rss/3544-thematique-societe-et-politiques-publiques.xml\",\n",
    "    \"Urbanisme, territoires et sols\": \"https://librairie.ademe.fr/rss/3509-thematique-urbanisme-territoires-et-sols.xml\"\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Classe Harvester (avec patch Prestashop)\n",
    "# ----------------------------\n",
    "class ShadowMassPDFHarvester:\n",
    "    def __init__(self, max_workers=10, db_path='shadow_harvester.db'):\n",
    "        self.session = requests.Session()\n",
    "        self.max_workers = max_workers\n",
    "        self.downloaded_urls = set()\n",
    "        self.failed_urls = set()\n",
    "        self.db_path = db_path\n",
    "        self.db_lock = threading.Lock()\n",
    "\n",
    "        # En-t√™tes HTTP\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "\n",
    "        # Base de donn√©es\n",
    "        self.init_database()\n",
    "\n",
    "    def init_database(self):\n",
    "        \"\"\"Initialisation de la base de donn√©es Shadow (thread-safe).\"\"\"\n",
    "        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS harvested_pdfs (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                url TEXT UNIQUE,\n",
    "                filename TEXT,\n",
    "                file_size INTEGER,\n",
    "                file_hash TEXT,\n",
    "                source_feed TEXT,\n",
    "                harvest_date TIMESTAMP,\n",
    "                status TEXT\n",
    "            )\n",
    "        ''')\n",
    "        self.conn.commit()\n",
    "\n",
    "    # ---------- D√©couverte / parsing g√©n√©rique (utiles si tu veux alimenter par sitemaps ou XML) ----------\n",
    "    def load_feeds_from_xml(self, xml_content_or_url):\n",
    "        \"\"\"Charge et parse les feeds XML de bases de donn√©es\"\"\"\n",
    "        feeds = []\n",
    "        try:\n",
    "            if isinstance(xml_content_or_url, str) and xml_content_or_url.startswith('http'):\n",
    "                response = self.session.get(xml_content_or_url, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                root = ET.fromstring(response.content)\n",
    "            else:\n",
    "                if isinstance(xml_content_or_url, str) and os.path.exists(xml_content_or_url):\n",
    "                    tree = ET.parse(xml_content_or_url)\n",
    "                    root = tree.getroot()\n",
    "                else:\n",
    "                    root = ET.fromstring(xml_content_or_url)\n",
    "\n",
    "            for item in root.iter():\n",
    "                feed_data = self._extract_feed_data(item)\n",
    "                if feed_data and feed_data.get('url'):\n",
    "                    feeds.append(feed_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SHADOW] Erreur parsing XML: {e}\")\n",
    "        return feeds\n",
    "\n",
    "    def _extract_feed_data(self, element):\n",
    "        \"\"\"Extraction agressive des donn√©es de feed\"\"\"\n",
    "        feed_data = {}\n",
    "\n",
    "        for attr, value in element.attrib.items():\n",
    "            if any(kw in attr.lower() for kw in ['url', 'link', 'href', 'src']):\n",
    "                feed_data['url'] = value\n",
    "            if 'title' in attr.lower():\n",
    "                feed_data['title'] = value\n",
    "\n",
    "        if element.text and element.text.strip():\n",
    "            if element.text.strip().startswith('http'):\n",
    "                feed_data['url'] = element.text.strip()\n",
    "            else:\n",
    "                feed_data['title'] = element.text.strip()\n",
    "\n",
    "        common_tags = {\n",
    "            'link': 'url', 'url': 'url', 'guid': 'url',\n",
    "            'title': 'title', 'description': 'description',\n",
    "            'pubdate': 'date', 'updated': 'date'\n",
    "        }\n",
    "\n",
    "        for child in element:\n",
    "            tag = child.tag.lower().split('}')[-1]\n",
    "            if tag in common_tags and child.text:\n",
    "                feed_data[common_tags[tag]] = child.text.strip()\n",
    "\n",
    "        return feed_data if feed_data.get('url') else None\n",
    "\n",
    "    def discover_feeds_from_sitemap(self, sitemap_url):\n",
    "        \"\"\"D√©couverte automatique de feeds via sitemaps\"\"\"\n",
    "        print(f\"[SHADOW] Exploration du sitemap: {sitemap_url}\")\n",
    "        try:\n",
    "            response = self.session.get(sitemap_url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            root = ET.fromstring(response.content)\n",
    "\n",
    "            namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "            urls = []\n",
    "\n",
    "            for url in root.findall('.//ns:url/ns:loc', namespace):\n",
    "                if url.text:\n",
    "                    urls.append({'url': url.text.strip(), 'source': 'sitemap'})\n",
    "\n",
    "            for sm in root.findall('.//ns:sitemap/ns:loc', namespace):\n",
    "                if sm.text:\n",
    "                    urls.extend(self.discover_feeds_from_sitemap(sm.text.strip()))\n",
    "            return urls\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERREUR] Sitemap exploration: {e}\")\n",
    "            return []\n",
    "\n",
    "    # ---------- Extraction / t√©l√©chargement ----------\n",
    "    def scan_url_for_pdfs(self, url, download_dir, title=\"\"):\n",
    "        \"\"\"Scan approfondi d'une URL pour trouver des PDFs\"\"\"\n",
    "        pdf_urls = set()\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            extraction_methods = [\n",
    "                self._extract_from_links,\n",
    "                self._extract_from_scripts,\n",
    "                self._extract_from_meta,\n",
    "                self._extract_from_iframes,\n",
    "                self._extract_from_data_attributes,\n",
    "                self._extract_from_json_ld,\n",
    "                self._extract_from_prestashop_scripts,  # PATCH : extraction cibl√©e Prestashop\n",
    "            ]\n",
    "            for method in extraction_methods:\n",
    "                try:\n",
    "                    pdf_urls.update(method(soup, url))\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] M√©thode {method.__name__} √©chou√©e: {e}\")\n",
    "\n",
    "            os.makedirs(download_dir, exist_ok=True)\n",
    "            downloaded = []\n",
    "            for pdf_url in pdf_urls:\n",
    "                if pdf_url not in self.downloaded_urls:\n",
    "                    if self._download_pdf_advanced(pdf_url, download_dir, title):\n",
    "                        self.downloaded_urls.add(pdf_url)\n",
    "                        downloaded.append(pdf_url)\n",
    "                        self._log_to_database(pdf_url, title, status='success')\n",
    "                    else:\n",
    "                        self.failed_urls.add(pdf_url)\n",
    "                        self._log_to_database(pdf_url, title, status='failed')\n",
    "            return downloaded\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERREUR] Scan de {url}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _extract_from_links(self, soup, base_url):\n",
    "        pdf_urls = set()\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href'].strip()\n",
    "            if 'controller=attachment' in href and 'id_attachment=' in href:\n",
    "                pdf_urls.add(urljoin(base_url, href))\n",
    "            elif self._is_pdf_link(href):\n",
    "                pdf_urls.add(urljoin(base_url, href))\n",
    "        return pdf_urls\n",
    "\n",
    "    def _extract_from_scripts(self, soup, base_url):\n",
    "        pdf_urls = set()\n",
    "        for script in soup.find_all('script'):\n",
    "            text = script.string or ''\n",
    "            patterns = [\n",
    "                r'[\"\\'](https?://[^\"\\']+?\\.pdf[^\"\\']*)[\"\\']',\n",
    "                r'(https?://[^\\s<>\"]+?\\.pdf[^\\s<>\"]*)',\n",
    "                r'pdfUrl[=:]\\s*[\"\\']([^\"\\']+\\.pdf[^\"\\']*)[\"\\']',\n",
    "                r'download[^=]*=\\s*[\"\\']([^\"\\']+?\\.pdf[^\"\\']*)[\"\\']'\n",
    "            ]\n",
    "            for pattern in patterns:\n",
    "                for match in re.findall(pattern, text, re.IGNORECASE):\n",
    "                    pdf_urls.add(urljoin(base_url, match))\n",
    "        return pdf_urls\n",
    "\n",
    "    def _extract_from_meta(self, soup, base_url):\n",
    "        pdf_urls = set()\n",
    "        for meta in soup.find_all('meta', content=True):\n",
    "            content = meta.get('content', '')\n",
    "            if isinstance(content, str) and '.pdf' in content.lower():\n",
    "                pdf_urls.add(urljoin(base_url, content))\n",
    "        return pdf_urls\n",
    "\n",
    "    def _extract_from_iframes(self, soup, base_url): t\n",
    "        pdf_urls = set()\n",
    "        for iframe in soup.find_all('iframe', src=True):\n",
    "            src = iframe['src']\n",
    "            if '.pdf' in src.lower():\n",
    "                pdf_urls.add(urljoin(base_url, src))\n",
    "        return pdf_urls\n",
    "\n",
    "    def _extract_from_data_attributes(self, soup, base_url):\n",
    "        \"\"\"PATCH : √©vite d'interpr√©ter un JSON entier comme URL, et reconstruit l'endpoint d'attachement.\"\"\"\n",
    "        pdf_urls = set()\n",
    "\n",
    "        def build_attachment_url(attach_id):\n",
    "            return urljoin(base_url, f\"/index.php?controller=attachment&id_attachment={attach_id}\")\n",
    "\n",
    "        for tag in soup.find_all(attrs=True):\n",
    "            for _, value in tag.attrs.items():\n",
    "                # Valeur simple (cha√Æne)\n",
    "                if isinstance(value, str):\n",
    "                    v = value.strip()\n",
    "\n",
    "                    # (1) Si c'est une URL/chemin plausible, traitement classique\n",
    "                    if (v.startswith('http') or v.startswith('/')) and self._is_pdf_link(v):\n",
    "                        pdf_urls.add(urljoin(base_url, v))\n",
    "                        continue\n",
    "\n",
    "                    # (2) Si √ßa ressemble √† du JSON Prestashop, on essaie d'en tirer id_attachment\n",
    "                    if v.startswith('{') and v.endswith('}'):\n",
    "                        try:\n",
    "                            data = json.loads(v)\n",
    "                            if isinstance(data, dict) and 'attachments' in data and isinstance(data['attachments'], list):\n",
    "                                for att in data['attachments']:\n",
    "                                    att_id = att.get('id_attachment') or att.get('id')\n",
    "                                    if att_id:\n",
    "                                        pdf_urls.add(build_attachment_url(att_id))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                # Valeur multiple (liste)\n",
    "                elif isinstance(value, (list, tuple)):\n",
    "                    for v in value:\n",
    "                        if isinstance(v, str) and (v.startswith('http') or v.startswith('/')) and self._is_pdf_link(v):\n",
    "                            pdf_urls.add(urljoin(base_url, v))\n",
    "\n",
    "        return pdf_urls\n",
    "\n",
    "    def _extract_from_json_ld(self, soup, base_url):\n",
    "        pdf_urls = set()\n",
    "        for script in soup.find_all('script', type='application/ld+json'):\n",
    "            try:\n",
    "                data = json.loads(script.string or '{}')\n",
    "                pdf_urls.update(self._find_pdf_in_json(data, base_url))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return pdf_urls\n",
    "\n",
    "    def _extract_from_prestashop_scripts(self, soup, base_url):\n",
    "        \"\"\"\n",
    "        PATCH : Cherche des blocs JSON Prestashop contenant 'attachments' et reconstruit les URLs de t√©l√©chargement.\n",
    "        \"\"\"\n",
    "        pdf_urls = set()\n",
    "\n",
    "        def build_attachment_url(attach_id):\n",
    "            return urljoin(base_url, f\"/index.php?controller=attachment&id_attachment={attach_id}\")\n",
    "\n",
    "        for script in soup.find_all('script'):\n",
    "            txt = script.string\n",
    "            if not txt or 'attachments' not in txt:\n",
    "                continue\n",
    "            # Tentative prudente d'extraction\n",
    "            # On ne force pas le parsing massif ; l'extraction data-attributes couvre d√©j√† la plupart des cas.\n",
    "            # Ici, on cherche simplement des id_attachment: \\d+\n",
    "            try:\n",
    "                # Cherche des patterns id_attachment dans le texte\n",
    "                ids = set(re.findall(r'id_attachment\"\\s*:\\s*(\\d+)', txt))\n",
    "                ids |= set(re.findall(r'id_attachment=\\s*(\\d+)', txt))\n",
    "                for att_id in ids:\n",
    "                    pdf_urls.add(build_attachment_url(att_id))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return pdf_urls\n",
    "\n",
    "    def _find_pdf_in_json(self, data, base_url):\n",
    "        pdf_urls = set()\n",
    "        if isinstance(data, dict):\n",
    "            for _, value in data.items():\n",
    "                if isinstance(value, str) and '.pdf' in value.lower():\n",
    "                    pdf_urls.add(urljoin(base_url, value))\n",
    "                else:\n",
    "                    pdf_urls.update(self._find_pdf_in_json(value, base_url))\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                pdf_urls.update(self._find_pdf_in_json(item, base_url))\n",
    "        return pdf_urls\n",
    "\n",
    "    def _is_pdf_link(self, href):\n",
    "        \"\"\"PATCH : √©vite les faux positifs JSON et accepte les endpoints d‚Äôattachements Prestashop.\"\"\"\n",
    "        if not isinstance(href, str):\n",
    "            return False\n",
    "        href = href.strip()\n",
    "\n",
    "        # Cas Prestashop: endpoint d‚Äôattachement\n",
    "        if 'controller=attachment' in href and 'id_attachment=' in href:\n",
    "            return True\n",
    "\n",
    "        # URLs ou chemins plausibles\n",
    "        if href.startswith('http') or href.startswith('/'):\n",
    "            parsed = urlparse(href)\n",
    "            path_has_pdf = parsed.path.lower().endswith('.pdf')\n",
    "            query_has_pdf = '.pdf' in (parsed.query or '').lower()\n",
    "            return path_has_pdf or query_has_pdf\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _download_pdf_advanced(self, pdf_url, download_dir, title=\"\"):\n",
    "        try:\n",
    "            # Nom de fichier\n",
    "            if title:\n",
    "                safe_title = re.sub(r'[^\\w\\-. ]', '', title).strip() or \"document\"\n",
    "                filename = f\"{safe_title}_{int(time.time())}.pdf\"\n",
    "            else:\n",
    "                filename = os.path.basename(urlparse(pdf_url).path) or f\"document_{int(time.time())}.pdf\"\n",
    "            filepath = os.path.join(download_dir, filename)\n",
    "\n",
    "            print(f\"üì• T√©l√©chargement: {pdf_url}\")\n",
    "            resp = self.session.get(pdf_url, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            content = resp.content\n",
    "            if not content.startswith(b'%PDF'):\n",
    "                print(f\"‚ö†Ô∏è  Non-PDF d√©tect√© (signature manquante): {pdf_url}\")\n",
    "                return False\n",
    "\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(content)\n",
    "\n",
    "            size = os.path.getsize(filepath)\n",
    "            if size < 1000:\n",
    "                os.remove(filepath)\n",
    "                print(f\"‚ö†Ô∏è  Fichier trop petit, supprim√©: {filename}\")\n",
    "                return False\n",
    "\n",
    "            print(f\"‚úÖ Sauvegard√©: {filename} ({size} bytes)\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå √âchec t√©l√©chargement {pdf_url}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _log_to_database(self, pdf_url, title, status):\n",
    "        \"\"\"Journalisation (prot√©g√©e par verrou)\"\"\"\n",
    "        try:\n",
    "            with self.db_lock:\n",
    "                cursor = self.conn.cursor()\n",
    "                file_hash = hashlib.md5(pdf_url.encode()).hexdigest()\n",
    "                cursor.execute('''\n",
    "                    INSERT OR REPLACE INTO harvested_pdfs \n",
    "                    (url, filename, file_size, file_hash, source_feed, harvest_date, status)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                ''', (pdf_url, title, 0, file_hash, 'rss_article_scan', datetime.now(), status))\n",
    "                self.conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"[DB ERROR] {e}\")\n",
    "\n",
    "    def generate_report(self):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute('SELECT status, COUNT(*) FROM harvested_pdfs GROUP BY status')\n",
    "        stats = dict(cursor.fetchall())\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üìä RAPPORT SHADOW HARVESTER\")\n",
    "        print(\"=\"*50)\n",
    "        for status, count in stats.items():\n",
    "            print(f\"   {status.upper()}: {count}\")\n",
    "        print(f\"\\nüéØ TOTAL PDFS (session): {len(self.downloaded_urls)}\")\n",
    "        print(f\"üí• √âCHECS (session): {len(self.failed_urls)}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Interface Jupyter + int√©gration Harvester\n",
    "# ----------------------------\n",
    "theme_selector = widgets.Dropdown(\n",
    "    options=list(FEEDS.keys()),\n",
    "    description='Th√®me :',\n",
    "    layout=widgets.Layout(width='60%')\n",
    ")\n",
    "\n",
    "btn_list = widgets.Button(description='Afficher les articles üì∞', button_style='success')\n",
    "btn_download = widgets.Button(description='Scanner & T√©l√©charger les PDFs ‚¨áÔ∏è', button_style='info')\n",
    "output = widgets.Output()\n",
    "\n",
    "display(theme_selector, widgets.HBox([btn_list, btn_download]), output)\n",
    "\n",
    "harvester = ShadowMassPDFHarvester(max_workers=10, db_path='shadow_harvester.db')\n",
    "_last_entries = []  # stocke les entr√©es list√©es pour r√©utilisation au t√©l√©chargement\n",
    "\n",
    "def _sanitize_dirname(name: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9_\\- ]', '_', name).strip().replace(' ', '_') or 'ademe'\n",
    "\n",
    "def on_list_clicked(_):\n",
    "    output.clear_output()\n",
    "    theme = theme_selector.value\n",
    "    url = FEEDS[theme]\n",
    "    with output:\n",
    "        print(f\"üîó Chargement du flux : {theme}\")\n",
    "        feed = feedparser.parse(url)\n",
    "        if not feed.entries:\n",
    "            print(\"‚ö†Ô∏è Aucun article trouv√©.\")\n",
    "            return\n",
    "        print(f\"‚úÖ {len(feed.entries)} article(s) trouv√©s :\\n\")\n",
    "        global _last_entries\n",
    "        _last_entries = feed.entries  # m√©morise pour le bouton t√©l√©chargement\n",
    "        for i, entry in enumerate(feed.entries, 1):\n",
    "            title = entry.get(\"title\", \"\")\n",
    "            link = entry.get(\"link\", \"\")\n",
    "            date = entry.get(\"published\", \"\")\n",
    "            display(Markdown(f\"**{i}. [{title}]({link})**\"))\n",
    "            if date:\n",
    "                display(Markdown(f\"_üìÖ {date}_\"))\n",
    "            print()\n",
    "\n",
    "def on_download_clicked(_):\n",
    "    theme = theme_selector.value\n",
    "    download_dir = f\"ademe_pdfs_{_sanitize_dirname(theme)}\"\n",
    "    with output:\n",
    "        if not _last_entries:\n",
    "            print(\"‚ÑπÔ∏è D‚Äôabord, clique sur ¬´ Afficher les articles ¬ª pour charger la liste.\")\n",
    "            return\n",
    "        print(f\"üöÄ Scan et t√©l√©chargement des PDFs li√©s aux {_sanitize_dirname(theme)} ‚Ä¶\")\n",
    "        total_found = 0\n",
    "        for i, entry in enumerate(_last_entries, 1):\n",
    "            title = entry.get(\"title\", \"\") or f\"article_{i}\"\n",
    "            link = entry.get(\"link\", \"\")\n",
    "            if not link:\n",
    "                continue\n",
    "            print(f\"\\n[{i}] üîé {title}\\nURL: {link}\")\n",
    "            found = harvester.scan_url_for_pdfs(link, download_dir, title=title)\n",
    "            print(f\"‚û°Ô∏è  PDFs trouv√©s: {len(found)}\")\n",
    "            total_found += len(found)\n",
    "\n",
    "        print(\"\\n‚Äî\" * 30)\n",
    "        print(f\"üì¶ Total PDFs t√©l√©charg√©s: {total_found} (dossier: {download_dir})\")\n",
    "        harvester.generate_report()\n",
    "        print(\"‚úÖ Termin√©.\")\n",
    "\n",
    "btn_list.on_click(on_list_clicked)\n",
    "btn_download.on_click(on_download_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8cbe09-844f-4c96-bec0-7faaabcbf1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
